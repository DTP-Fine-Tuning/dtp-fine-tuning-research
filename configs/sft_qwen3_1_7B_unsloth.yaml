# Unsloth Training Configuration for Qwen3-1.7B
# Optimized for fast training with instruction masking

# Model Configuration
model:
  name: "Qwen/Qwen3-1.7B"
  trust_remote_code: true
  use_cache: false

# Tokenizer Configuration
tokenizer:
  padding_side: "left"  # Qwen3 uses left padding
  trust_remote_code: true

# Quantization Configuration (4-bit for memory efficiency)
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true

# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  target_modules:  # Qwen3 specific modules
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Dataset Configuration
dataset:
  name: "DTP2/dtp-multiturn-indonesian-interview-1k"
  split: "train"
  test_size: 0.1  # 10% for evaluation
  seed: 42
  max_length: 2048  # Reduce if OOM

# Training Arguments
training:
  output_dir: "./SFT-Qwen3-1.7B-Unsloth"
  run_name: "SFT-Qwen3-1.7B-Unsloth-MultiTurn"

  # ‚≠ê IMPORTANT: Enable instruction masking (train only on assistant responses)
  assistant_only_loss: true

  # Batch Configuration
  per_device_train_batch_size: 2  # Reduce to 1 if OOM
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4  # Effective batch size = 2 * 4 = 8

  # Training Parameters
  num_train_epochs: 3
  max_steps: -1  # -1 means train for full epochs, or set to specific number for testing
  learning_rate: 2e-4
  warmup_ratio: 0.03
  warmup_steps: 0

  # Optimizer (8-bit for memory efficiency)
  optim: "adamw_8bit"
  weight_decay: 0.01

  # Scheduler
  lr_scheduler_type: "linear"

  # Logging & Evaluation
  logging_steps: 2  # Log every step (set to 10 for less verbose)
  eval_strategy: "steps"
  eval_steps: 4  # Evaluate every 100 steps
  save_strategy: "steps"
  save_steps: 4  # Save checkpoint every 100 steps
  save_total_limit: 2  # Keep only 2 best checkpoints

  # Model Selection
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  # Misc
  seed: 42
  report_to: "wandb"  # or "none" to disable WandB
  dataset_num_proc: 2  # Number of processes for dataset preprocessing
  packing: false  # Packing can be faster but may affect quality

# Callbacks Configuration
callbacks:
  early_stopping:
    enabled: true
    patience: 3  # Stop if no improvement for 3 evaluations
    threshold: 0.0

  custom_logging:
    enabled: true

  memory_monitor:
    enabled: true
    log_every_n_steps: 4

  save_best_model:
    enabled: true

# Chat Template Configuration
chat_template:
  # Unsloth will auto-detect Qwen template
  # You can override with custom template if needed:
  # template_string: |
  #   {% for message in messages %}
  #   ...
  #   {% endfor %}

  # System message (optional)
  system_message: "Anda adalah asisten AI yang membantu dan informatif."
  use_system_message: false  # Set to true to add system message to all conversations

# Evaluation Configuration (NEW in Unsloth script!)
evaluation:
  metrics:
    - "bleu"
    - "rouge"
  test_samples: 100  # Number of samples to evaluate with BLEU/ROUGE
  save_predictions: true  # Save predictions to JSON file
  output_dir: "./evaluation_results"

# Inference Configuration (for evaluation)
inference:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.95
  repetition_penalty: 1.3  # Prevent repetition

# Weights & Biases Configuration
wandb:
  entity: "DTP2"
  project: "SFT_Interview_Experiment" # Your project name
  tags:
    - "qwen3"
    - "qwen3-1.7b"
    - "unsloth"
    - "multi-turn"
    - "indonesian"
    - "instruction-masking"
  notes: "Fine-tuning Qwen3-1.7B with Unsloth optimization and instruction masking"

# Paths
paths:
  final_model_dir: "./SFT-Qwen3-1.7B-Unsloth-final"
  checkpoints_dir: "./checkpoints"
  logs_dir: "./logs"

# Model Saving Options
save_merged_model: false  # Set to true to save merged model (base + LoRA)
                          # Warning: Merged model is much larger!

# Push to Hub (optional)
push_to_hub: false
hub_model_id: ""  # e.g., "username/model-name"

# Advanced Training Options (optional)
advanced:
  use_flash_attention: false  # Enable if supported
  use_lora_plus: false
  use_neftune: false
  neftune_noise_alpha: 5.0
  max_grad_norm: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
