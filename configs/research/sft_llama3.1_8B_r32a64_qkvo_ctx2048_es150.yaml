# Training Configuration for Llama 3.1 SFT
# Project: dtp-fine-tuning-research

# Model Configuration
model:
  name: "meta-llama/Llama-3.1-8B"
  trust_remote_code: true
  use_cache: false

# Tokenizer Configuration
tokenizer:
  padding_side: "right"
  trust_remote_code: true

# Quantization Configuration (BitsAndBytes)
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true
# LoRA Configuration
lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

  task_type: "CAUSAL_LM"
# Dataset Configuration
dataset:
  name: "dtp-fine-tuning/dtp-multiturn-interview-1k-combined"
  split: "train"
  test_size: 0.05
  seed: 42
  text_field: "text"
  max_length: 2048
# Training Arguments
training:
  output_dir: "./SFT-Llama-3.1-8B_r32a64-qkvo_ctx2048_es150"
  run_name: "SFT-Llama-3.1-8B_r32a64-qkvo_ctx2048_es150"
  
  # Batch Configuration
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  
  # Memory Optimization
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  
  # Training Parameters
  num_train_epochs: 4
  learning_rate: 0.0002
  warmup_ratio: 0.03
  
  # Optimizer
  optim: "paged_adamw_8bit"
  weight_decay: 0.01
  
  # Scheduler
  lr_scheduler_type: "cosine"
  
  # Precision
  bf16: false
  fp16: true
  
  # Logging & Evaluation
  logging_steps: 25
  eval_strategy: "steps"
  eval_steps: 150
  save_strategy: "steps"
  save_steps: 150
  save_total_limit: 2
  
  # Data Configuration
  packing: false
  dataset_text_field: "text"
  dataloader_num_workers: 0
  remove_unused_columns: true
  
  # Model Selection
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # Misc
  seed: 42
  report_to: "wandb"
# Callback Configuration
callbacks:
  early_stopping:
    enabled: true
    patience: 2
    threshold: 0.0
  
  custom_logging:
    enabled: true
  
  memory_monitor:
    enabled: true
    log_every_n_steps: 100
  
  save_best_model:
    enabled: true
# Chat Template Configuration
chat_template:
  system_message: "Anda adalah interviewer dari platform talenta digital. Tugas Anda adalah menggali detail pendidikan, tugas akhir, sertifikasi/pelatihan, dan pengalaman kerja talenta lewat percakapan tanya jawab."
  use_llama_format: true
# Weights & Biases Configuration
wandb:
  entity: "DTP2"
  project: "SFT_Interview_Experiment"
# Paths (relative to project root)
paths:
  final_model_dir: "./SFT-Llama-3.1-8B_r32a64-qkvo_ctx2048_es150/final_model"
